# 13_XGBoost_LightGBM

## ğŸ“Œ Module Overview
This module focuses on **Gradient Boostingâ€“based algorithms**, specifically **XGBoost** and **LightGBM**, which are powerful ensemble techniques widely used in machine learning competitions and real-world applications.

---

## ğŸ§  Key Concepts Covered
- Boosting vs Bagging  
- Gradient Boosting fundamentals  
- Sequential tree learning  
- Learning rate and tree depth  
- Regularization in boosting  

---

## ğŸ“Š Topics Included
- Working of XGBoost  
- Working of LightGBM  
- Differences between XGBoost and LightGBM  
- Handling large datasets efficiently  
- Reducing overfitting using boosting  

---

## âš™ï¸ Workflow Followed
1. Import required libraries  
2. Load and explore dataset  
3. Perform data preprocessing  
4. Split data into training and testing sets  
5. Train XGBoost and LightGBM models  
6. Make predictions  
7. Compare model performance  

---

## ğŸ“ˆ Evaluation Metrics Used
- Accuracy  
- RMSE  
- RÂ² Score  
- Confusion Matrix (for classification tasks)  

---

## ğŸš€ Skills Demonstrated
- Advanced ensemble modeling  
- Boosting-based optimization  
- Model comparison and tuning  
- Working with high-performance ML models  

---

## â­ Why This Module Matters
- Frequently used in Kaggle competitions  
- High accuracy and scalability  
- Industry-standard algorithms  
- Essential for advanced ML roles  

---

## âœ… Status
âœ”ï¸ Completed  

---

## ğŸ”œ Next Module
â¡ï¸ **14_PCA**

---

