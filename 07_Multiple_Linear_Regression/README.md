# 07_Multiple_Linear_Regression â€“ Car Price Prediction

## ğŸ“Œ Overview
This module demonstrates **Multiple Linear Regression (MLR)** to predict car prices using multiple numerical and categorical features.  
A real-world automobile dataset is used to build, evaluate, and improve regression models through **feature selection, scaling, and regularization**.

---

## ğŸ“‚ Dataset Used
- **ToyotaCorolla - MLR.csv**
- Target Variable: **Price**
- Features include:
  - Age, KM driven, Fuel Type, Horse Power
  - Engine capacity, Doors, Weight, Gears, etc.

---

## ğŸ› ï¸ Libraries Used
- `pandas`
- `numpy`
- `matplotlib`
- `scikit-learn`

---

## ğŸ” Workflow

### 1ï¸âƒ£ Data Loading & Inspection
- Loaded dataset using Pandas
- Checked data types, missing values, and statistical summary
- Ensured dataset quality before modeling

---

### 2ï¸âƒ£ Exploratory Analysis
- **Boxplots** to detect outliers
- **Scatter plots** to analyze relationships between predictors and price
- Correlation analysis to understand feature importance

---

### 3ï¸âƒ£ Data Preprocessing
- **Label Encoding** for categorical variable (`Fuel_Type`)
- **Standardization** using `StandardScaler`
- Combined scaled features with target variable for analysis

---

### 4ï¸âƒ£ Feature Selection
- Removed low-correlation features based on correlation values
- Built multiple models by progressively dropping weak predictors

---

## ğŸ“ˆ Model Building & Evaluation

### ğŸ”¹ Model 1
- Removed `Cylinders` due to weak correlation  
- **RMSE:** 1359.147  
- **RÂ² Score:** 0.845  

### ğŸ”¹ Model 2 (Best Model)
- Removed low-impact features (`Cylinders`, `Fuel_Type`, `Automatic`, `Gears`)  
- **RMSE:** 1337.144  
- **RÂ² Score:** 0.850  

### ğŸ”¹ Model 3
- Further feature reduction  
- **RMSE:** 1352.954  
- **RÂ² Score:** 0.846  

ğŸ“Œ **Model 2 performed best** with the lowest RMSE and highest RÂ² score.

---

## ğŸ”§ Regularization Techniques

### âœ”ï¸ Lasso Regression (L1)
- Helps with feature selection
- Shrinks less important coefficients to zero

### âœ”ï¸ Ridge Regression (L2)
- Reduces multicollinearity
- Controls large coefficient values

Lasso and Ridge models were applied to **all three MLR models** for comparison.

---

## ğŸ“˜ Conceptual Understanding

### ğŸ”¹ Normalization vs Standardization
- **Normalization:** Scales data between 0 and 1
- **Standardization:** Converts data to mean = 0 and std = 1
- Standardization is preferred for regression models

---

### ğŸ”¹ Handling Multicollinearity
- Drop highly correlated features
- Use **Variance Inflation Factor (VIF)**
- Apply **Ridge or Lasso Regression**

---

## â­ Why This Module Matters
- Builds strong foundations in **predictive modeling**
- Demonstrates real-world **feature selection**
- Teaches **model evaluation & comparison**
- Introduces **regularization techniques**
- Essential for regression-based ML problems

---

## ğŸ”œ Next Module
â¡ï¸ **Logistic Regression**

---

## ğŸš€ Skills Demonstrated
- Multiple Linear Regression
- Feature encoding & scaling
- Correlation-based feature selection
- Model evaluation (RMSE, RÂ²)
- Lasso & Ridge regularization
- Practical ML workflow

---

## âœ… Status
âœ” Regression models built  
âœ” Best model selected  
âœ” Regularization applied  
âœ” Ready for classification models

